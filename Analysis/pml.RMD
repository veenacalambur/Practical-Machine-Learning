---
title: "Exercise Classification"
author: "Veena Calambur"
date: "January 31, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(ggplot2)
library(randomForest)
library(dplyr)
library(lazyeval)
set.seed(12345)
```

Read in our training and testing data, and further split our training data to test various models before touching our testing set. 
```{r read Data, echo = TRUE}
setwd("~/Data Science Training/Practical Machine Learning/Input Data")
train = read.csv("pml-training.csv")
test = read.csv("pml-testing.csv")

dim(train); dim(test);

#split train for train/testing purpose (train.train and train.test)
train.inTrain = createDataPartition(train$classe, p = 0.6, list = FALSE) 
train.train = train[train.inTrain,]
train.test = train[-train.inTrain,]

dim(train.train); dim(train.test)
```
Cleaning data - look for and remove "near zero variables", imputing missing values with KNN and center and scale the values - note we will have to repeat this to our testing sets: 
```{r throw random models around, echo = TRUE}
# function to throw out NA values for a given DF 
find.NA.vars <- function(df){
  num_vars = ncol(df)
  include = c()
  for(i in names(df)){
    filter.cond = interp(~ (!is.na(var1)), var1 = as.name(i))
    if(nrow(df%>%filter_(filter.cond))/nrow(df) > 0.75){
      include = c(include, i)
    }
  }
  return(include)
}

# determine values to be tossed out 
var.include <- find.NA.vars(train.train)
train.train.noNA <- train.train[, var.include]

# find all "near zero variables" and remove those from the train / test set 
train.train.NZV <- nearZeroVar(train.train.noNA, saveMetrics=TRUE)
train.train.NZV.varlist = row.names(train.train.NZV[train.train.NZV$nzv == FALSE,])
train.train.cleaned <- train.train.noNA[,train.train.NZV.varlist]

train.test.cleaned <- train.test[,train.train.NZV.varlist]

# preProcess data by normalizing the data - knnIMmputes, center and scale the values 
preProcess.tree1.varlist = which(lapply(train.train.cleaned, class) %in% "numeric")
preProcess.tree1.Obj = preProcess(train.train.cleaned[,preProcess.tree1.varlist],method=c('knnImpute','center', 'scale'))

# process the train / test data sets: 
train.train.tree1.processed = predict(preProcess.tree1.Obj, train.train.cleaned[,preProcess.tree1.varlist])
train.train.tree1.processed$classe = train.train.cleaned$classe

train.test.tree1.processed = predict(preProcess.tree1.Obj, train.test.cleaned[,preProcess.tree1.varlist])
train.test.tree1.processed$classe = train.test$classe

```

```{r modelTREE, echo = TRUE}

modFit.tree1 <- train(classe ~ ., data =  train.train.tree1.processed, method="rpart")
modFit.tree1
confusionMatrix(train.test$classe, predict(modFit.tree1, train.test.tree1.processed))

```
The in-sample and out of sample rates are pretty low with a single deicsion tree. Let's try a proper random forest. Note that for random forests, there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error.

```{r processing preRF, echo = FALSE}
# determine values to be tossed out 
var.include <- find.NA.vars(train)
train.noNA <- train[, var.include]

# find all "near zero variables" and remove those from the train / test set 
train.NZV <- nearZeroVar(train.noNA, saveMetrics=TRUE)
train.NZV.varlist = row.names(train.NZV[train.NZV$nzv == FALSE,])
train.cleaned <- train.noNA[,train.NZV.varlist]

test.cleaned <- test[,train.NZV.varlist[-59]]

# preProcess data by normalizing the data - knnIMmputes, center and scale the values 
preProcess.rf1.varlist = which(lapply(train.cleaned, class) %in% "numeric")
preProcess.rf1.Obj = preProcess(train.cleaned[,preProcess.rf1.varlist],method=c('knnImpute','center', 'scale'))

# process the train / test data sets: 
train.rf1.processed = predict(preProcess.rf1.Obj, train.cleaned[,preProcess.rf1.varlist])
train.rf1.processed$classe = as.factor(train.cleaned$classe)

test.rf1.processed = predict(preProcess.rf1.Obj, test.cleaned[,preProcess.rf1.varlist])

```

Method 2: Random Forest 

```{r processing RF, echo = TRUE}
# fit the model
modFit.rf1 <- randomForest(classe ~ ., data=train.rf1.processed,importance=T)
modFit.rf1

# test prediction: 
test_prediction = predict(modFit.rf1, test.rf1.processed)

```

This estimate is definitely better than our single decision tree! I know there are a few issues where some variables are actually directly related to the classe variable. I would consider explore PCA to potentially reduce this variability in the model in future iterations of this project. 